{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "os.chdir(\"/content/drive/\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB0npoK2B1rN",
        "outputId": "0271ffbe-f92c-45aa-c923-261989a50021"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "MyDrive  Shareddrives\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# def remove_columns_with_max_except_thread_duration_max(file_path):\n",
        "#     # Read the CSV file into a pandas DataFrame\n",
        "#     df = pd.read_csv(file_path)\n",
        "\n",
        "#     # Get a list of columns to drop\n",
        "#     columns_to_drop = [col for col in df.columns if \"_max\" in col or \"_avg\" in col or \"remote\" in col and col != \"thread_duration_max\"]\n",
        "\n",
        "#         # Print the columns to be removed\n",
        "#     print(\"Columns to be removed:\")\n",
        "#     for col in columns_to_drop:\n",
        "#         print(col)\n",
        "\n",
        "#     # Drop the columns from the DataFrame\n",
        "#     df.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "#     # Save the updated DataFrame back to a new CSV file\n",
        "#     new_file_path = file_path.replace(\".csv\", \"_updated.csv\")\n",
        "#     df.to_csv(new_file_path, index=False)\n",
        "\n",
        "#     return new_file_path\n",
        "\n",
        "# # Replace 'file_path.csv' with the path to your CSV file\n",
        "# csv_file_path = '/content/drive/MyDrive/Colab Notebooks/data.csv'\n",
        "# updated_file_path = remove_columns_with_max_except_thread_duration_max(csv_file_path)\n",
        "# print(f\"Columns with '_max' and '_avg' removed, except 'thread_duration_max', in the file '{csv_file_path}' and saved to '{updated_file_path}'.\")\n"
      ],
      "metadata": {
        "id": "MZsLZHWk96lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def remove_columns(file_path):\n",
        "    # Columns to be removed\n",
        "    columns_to_drop = [\n",
        "        \"updated_on\", \"is_deleted\", \"query_hash\", \"status\", \"workspace_id\", \"workspace\",\n",
        "        \"catalog\", \"cluster_uuid\", \"planType\", \"plan\", \"error\", \"database\", \"added_on\",\n",
        "        \"email\", \"operator_id\", \"table_name\", \"cluster_name\"\n",
        "    ]\n",
        "\n",
        "    # Read the CSV file into a pandas DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Get a list of columns to drop (with '_max' but not 'thread_duration_max' and additional columns)\n",
        "    columns_to_drop += [col for col in df.columns if \"_max\" in col or \"_avg\" in col or \"remote\" in col and col != \"thread_duration_max\"]\n",
        "\n",
        "    # Print the columns to be removed\n",
        "    print(\"Columns to be removed:\")\n",
        "    for col in columns_to_drop:\n",
        "        print(col)\n",
        "\n",
        "    # Drop the columns from the DataFrame\n",
        "    df.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "    # Save the updated DataFrame back to a new CSV file\n",
        "    new_file_path = file_path.replace(\".csv\", \"_updated.csv\")\n",
        "    df.to_csv(new_file_path, index=False)\n",
        "\n",
        "    return new_file_path\n",
        "\n",
        "# Replace 'file_path.csv' with the path to your CSV file\n",
        "csv_file_path = '/content/drive/MyDrive/Colab Notebooks/data.csv'\n",
        "updated_file_path = remove_columns(csv_file_path)\n",
        "print(f\"Columns removed in the file '{csv_file_path}' and saved to '{updated_file_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxPA6IkLGG52",
        "outputId": "3c526f90-588c-4866-cd01-c839c26f5264"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-1ff453f30530>:12: DtypeWarning: Columns (11,12,13,14,75,81,87,124,130) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns to be removed:\n",
            "updated_on\n",
            "is_deleted\n",
            "query_hash\n",
            "status\n",
            "workspace_id\n",
            "workspace\n",
            "catalog\n",
            "cluster_uuid\n",
            "planType\n",
            "plan\n",
            "error\n",
            "database\n",
            "added_on\n",
            "email\n",
            "operator_id\n",
            "table_name\n",
            "cluster_name\n",
            "file_name_max_read_time\n",
            "thread_duration_max\n",
            "RowsInPerThread_max\n",
            "remoteParallelism\n",
            "remote_num_chunks\n",
            "SinkOperator_avg\n",
            "LogicalValuesOperator_avg\n",
            "ChunkAppendAndSortOperator_avg\n",
            "ProjectionOperator_avg\n",
            "JoinOperator_avg\n",
            "ColumnarLookupPartBuildOperator_max\n",
            "InMemoryAggregationOperator_avg\n",
            "JoinOperator_max\n",
            "ChunkAppendOperator_avg\n",
            "ProjectionOperator_max\n",
            "ColumnarLookupTableBuildOperator_avg\n",
            "PartInMemoryAggregationOperator_max\n",
            "TableScanOperator_max\n",
            "PartInMemoryAggregationOperator_avg\n",
            "ColumnarLookupPartBuildOperator_avg\n",
            "TableScanOperator_avg\n",
            "LimitOperator_avg\n",
            "InMemoryWindowOperator_avg\n",
            "InMemoryPartWindowOperator_avg\n",
            "FilterOperator_avg\n",
            "InMemoryAggregationOperator_max\n",
            "InMemoryPartWindowOperator_max\n",
            "InMemoryWindowOperator_max\n",
            "FilterOperator_max\n",
            "ChunkAppendOperator_max\n",
            "UnionOperator_max\n",
            "UnionOperator_avg\n",
            "remote_cost_percent\n",
            "remote_cost_percent_str_Percentage\n",
            "remote_cost_percent_str_Values\n",
            "PartSortWithLimitOperator_avg\n",
            "SortWithLimitOperator_avg\n",
            "Columns removed in the file '/content/drive/MyDrive/Colab Notebooks/data.csv' and saved to '/content/drive/MyDrive/Colab Notebooks/data_updated.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def merge_and_drop_local_columns(file_path):\n",
        "    # Read the CSV file into a pandas DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Find all columns that have names starting with \"local_\"\n",
        "    local_columns = [col for col in df.columns if col.startswith(\"local_\")]\n",
        "\n",
        "    # Process each local column\n",
        "    for local_col in local_columns:\n",
        "        # Get the corresponding non-local column name (removing \"local_\" prefix)\n",
        "        non_local_col = local_col.replace(\"local_\", \"\")\n",
        "\n",
        "        # Merge data from local column with non-local column where the data is mutually exclusive\n",
        "        df[non_local_col] = df.apply(lambda row: row[non_local_col] if pd.notnull(row[local_col]) else row[non_local_col], axis=1)\n",
        "\n",
        "    # Drop the local columns\n",
        "    df.drop(columns=local_columns, inplace=True)\n",
        "\n",
        "    # Save the updated DataFrame back to a new CSV file\n",
        "    new_file_path = file_path.replace(\".csv\", \"_updated.csv\")\n",
        "    df.to_csv(new_file_path, index=False)\n",
        "\n",
        "    return new_file_path\n",
        "\n",
        "# Replace 'file_path.csv' with the path to your CSV file\n",
        "csv_file_path = '/content/drive/MyDrive/Colab Notebooks/data_updated.csv'\n",
        "updated_file_path = merge_and_drop_local_columns(csv_file_path)\n",
        "print(f\"Data merged and local columns dropped in the file '{csv_file_path}' and saved to '{updated_file_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25kdLI5AoUi4",
        "outputId": "9966f0e9-515a-4ba6-bfdc-674248db6461"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-c6836df8ed9f>:5: DtypeWarning: Columns (5,63,70,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data merged and local columns dropped in the file '/content/drive/MyDrive/Colab Notebooks/data_updated.csv' and saved to '/content/drive/MyDrive/Colab Notebooks/data_updated_updated.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to check each query ID's sink operator if below query level metrics exist or not, if it exists, then it should reflect on all operators which have the same query id"
      ],
      "metadata": {
        "id": "isaK5RdPBB7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "def validate_data_consistency(df):\n",
        "\n",
        "    # Group the DataFrame by 'query_id' and apply a custom function to check data consistency\n",
        "    grouped = df.groupby('query_id')\n",
        "    for name, group in grouped:\n",
        "        cols_to_check = [\"total_query_time\", \"queue_blocked_time_Values\", \"parsingTime\",\n",
        "                         \"totalClientQueryTime\", \"totalOpenDuration\", \"totalBytes\"]\n",
        "\n",
        "        unique_values = {}\n",
        "        for col in cols_to_check:\n",
        "            unique_values[col] = group[col].dropna().unique()\n",
        "\n",
        "        # Check if more than one unique value exists for any of the specified columns\n",
        "        if any(len(values) > 1 for values in unique_values.values()):\n",
        "            raise ValueError(f\"Data inconsistency found for query_id: {name}\")\n",
        "\n",
        "        # Fill missing values in the group with the common data for the specified columns\n",
        "        common_data = {}\n",
        "        for col, values in unique_values.items():\n",
        "            if len(values) == 1:\n",
        "                common_data[col] = values[0]\n",
        "\n",
        "        df.loc[group.index, cols_to_check] = df.loc[group.index, cols_to_check].fillna(common_data)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Assuming you have already read the CSV file into the 'df' DataFrame\n",
        "# ...\n",
        "\n",
        "# Merge data from local columns as done previously\n",
        "# ...\n",
        "\n",
        "# Validate data consistency for each query_id and fill missing values\n",
        "df = validate_data_consistency(df)\n",
        "\n",
        "# Save the updated DataFrame back to a new CSV file\n",
        "csv_file_path = '/content/drive/MyDrive/Colab Notebooks/data_updated_updated.csv'\n",
        "new_file_path = csv_file_path.replace(\".csv\", \"_updated.csv\")\n",
        "df.to_csv(new_file_path, index=False)\n",
        "\n",
        "print(f\"Data merged, local columns dropped, and data consistency validated in the file '{csv_file_path}' and saved to '{new_file_path}'.\")\n"
      ],
      "metadata": {
        "id": "Sa3z2RqNB8qO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b9c4ae-ca69-49aa-9b3a-fdd0691ccf89"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-499f282bae3f>:1: DtypeWarning: Columns (5,63,70,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(csv_file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data merged, local columns dropped, and data consistency validated in the file '/content/drive/MyDrive/Colab Notebooks/data_updated_updated.csv' and saved to '/content/drive/MyDrive/Colab Notebooks/data_updated_updated_updated.csv'.\n"
          ]
        }
      ]
    }
  ]
}